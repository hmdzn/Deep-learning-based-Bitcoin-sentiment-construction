{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Deep learning-based cryptocurrency\nsentiment construction**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/df.csv\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_missing_data(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        # written by MJ Bahmani\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_missing_data(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('class').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspect sentiment\nsns.countplot(df['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Processing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nimport re\nimport string\ndef preprocess(s,remove_stopwords=False):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    \n    # Convert words to lower case \n    s = s.lower()\n\n    # Clean the text with the same procedure\n    s = re.sub(r\"http\\S+\", \"linktag\", s) #linktag\n    s = re.sub(r\"@\\S+\", \"usertag\", s) #usertag\n    s = re.sub(r\"$\\S+\", \"moneytag\", s) #moneytag\n    #s = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \"numbertag\", s) #numbertag\n    s = re.sub(r\" not \", \" negtag_ \", s) #negtag_ added to \"not\", \"no\", \"none\",\"neither\", \"never\" \"nobody\"\n    s = re.sub(r\" none \", \" negtag_ \", s)\n    s = re.sub(r\" no \", \" negtag_ \", s)\n    s = re.sub(r\" neither \", \" negtag_ \", s)\n    s = re.sub(r\" never \", \" negtag_ \", s)\n    s = re.sub(r\" nobody \", \" negtag_ \", s)\n    s = re.sub(r\"(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)\", \" \", s) # \n    s = re.sub(r\"&amp\", \" \", s) #\n    s = re.sub(r\" RT \", \" \", s) #\n    \n    # remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        s = [w for w in s if not w in stops]\n\n    # remove punctuation except \"!\" and \"?\" from each word\n    remove = string.punctuation\n    remove = remove.replace(\"?\", \"\") # don't remove Interrogation marks\n    remove = remove.replace(\"!\", \"\") # don't remove Exclamation marks\n    pattern = r\"[{}]\".format(remove) # create the pattern\n    re.sub(pattern, \"\", s) \n    \n    # Return a list of words\n    return(s)\ndf['clean_tweet'] = df['tweets'].apply(preprocess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On affiche les tweets contenant ? ou !\nfor i in range(df.shape[0]):\n    if('?' in df.iloc[i,0] or '!' in df.iloc[i,0]):\n        print(i,' ',df.iloc[i,0],'||',df.iloc[i,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\ntext = df['clean_tweet'].to_string().lower()    \nwordcloud = WordCloud(\n    collocations=False,\n    relative_scaling=0.5,\n    stopwords=set(stopwords.words('english'))).generate(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df['clean_tweet'].to_string().lower()    \nwordcloud = WordCloud(\n    collocations=False,\n    relative_scaling=0.5,\n    stopwords=set(stopwords.words('english'))).generate(text)\n\nplt.figure(figsize=(12,12))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Une première remarque : on voit linktag est très bien présentée, du coup peut être qu'il n'était pas judicieux d'uniformiser les liens puisqu'ils n'ont peut être pas la même valeur"},{"metadata":{},"cell_type":"markdown","source":"**RNN algorithm setup**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten,Reshape\nfrom keras.layers import Conv1D, MaxPooling1D\nfrom keras.utils import np_utils\nfrom keras.layers import LSTM, LeakyReLU\nfrom keras.callbacks import CSVLogger, ModelCheckpoint\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport h5py\nimport os\nimport tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RNN without pretraining**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(df.shape[0]):\n    if(df[\"class\"][i]==\"['positive']\"):\n        df[\"class\"][i]=1\n    if(df[\"class\"][i]==\"['negative']\"):\n        df[\"class\"][i]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna(subset=['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 201 # fix random seed for reproducibility\nnp.random.seed(seed)\nX, y = (df[\"clean_tweet\"].values, df[\"class\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\ntk = Tokenizer(lower = True)\ntk.fit_on_texts(X)\nX_seq = tk.texts_to_sequences(X)\nX_pad = pad_sequences(X_seq, maxlen=256, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pad=pd.DataFrame(X_pad)\nX_pad.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_pad[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pad.shape,df.shape,y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_n = X_pad[df['class']==0]\nX_p = X_pad[df['class']==1]\ny_p = y[df['class']==1]\ny_n = y[df['class']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_p.shape,y_p.shape,X_n.shape,y_n.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split Train Test sets, en essayant d'avoir des proportions équilibrées de \"positive\" & \"negative\"\n\nX_p_train, X_p_test,y_p_train,y_p_test = train_test_split(X_p,y_p, test_size=0.3, random_state=seed)\n\nX_n_train, X_n_test,y_n_train,y_n_test = train_test_split(X_n,y_n, test_size=0.3, random_state=seed)\n\nX_p_train=pd.DataFrame(X_p_train)\nX_n_train=pd.DataFrame(X_n_train)\nX_train=X_p_train.append(X_n_train)  \n\ny_p_train=pd.DataFrame(y_p_train)\ny_n_train=pd.DataFrame(y_n_train)\ny_train=y_n_train.append(y_p_train)\n\nX_p_test=pd.DataFrame(X_p_test)\nX_n_test=pd.DataFrame(X_n_test)\nX_test=X_p_test.append(X_n_test)\n\ny_p_test=pd.DataFrame(y_p_test)\ny_n_test=pd.DataFrame(y_n_test)\ny_test=y_p_test.append(y_n_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\nX_train1 = X_train[batch_size:]\ny_train1 = y_train[batch_size:]\nX_valid = X_train[:batch_size]\ny_valid = y_train[:batch_size]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nmax_words = 100\nembedding_size = 100\nmodel = Sequential()\nmodel.add(Embedding(20000, embedding_size, input_length=max_words))\nmodel.add(LSTM(64))\nmodel.add(Dense(1, activation='tanh'))\nmodel.compile(loss='binary_crossentropy', optimizer='Adadelta', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_history = model.fit(X_train1, y_train1, validation_data=(X_valid, y_valid),epochs=50, batch_size=batch_size, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores=model.evaluate(X_test,y_test,verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores[1]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, GRU, Dense, Dropout\n\nmax_words = 100\nembedding_size = 32\nmodel = Sequential()\nmodel.add(Embedding(20000, embedding_size, input_length=max_words))\nmodel.add(keras.layers.GRU(128))\nmodel.add(Dense(1, activation='tanh'))\nmodel.compile(loss='binary_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\nmodel_history = model.fit(X_train1, y_train1, validation_data=(X_valid, y_valid),epochs=50, batch_size=batch_size, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores=model.evaluate(X_test,y_test,verbose=0)\nprint(scores[1]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?Sequential().add(Embedding())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# je trouve aussi 0.7931312665667858 comme accuracy\n# en variant le paramètre vocabulary_size on voit qu'il a une importance forte sur le score final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RNN pretrained with Word2Vec Skip-gram**"},{"metadata":{},"cell_type":"markdown","source":"> Skip gram"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import brown\nfrom gensim.models import Word2Vec\nimport multiprocessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import KeyedVectors\n#model_ug_cbow = KeyedVectors.load('w2v_model_ug_cbow.word2vec')\n#model_ug_sg = KeyedVectors.load('w2v_model_ug_sg.word2vec')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" from nltk import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# on met en places les sentences à partir des quelles on va construire le vocabulaire\nfrom nltk import word_tokenize\nfrom gensim.models.phrases import Phrases, Phraser\nsent = [row.split() for row in df['clean_tweet']]\nphrases = Phrases(sent, max_vocab_size = 50, progress_per=10000)\nbigram = Phraser(phrases)\nsentences = bigram[sent]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences.corpus[0][-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?Phrases()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict  \nword_freq = defaultdict(int)\nfor sent in sentences:\n    for i in sent:\n        word_freq[i] += 1\nlen(word_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(word_freq, key=word_freq.get, reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another method"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import LabeledSentence\nimport multiprocessing\nfrom sklearn import utils\n\nfrom sklearn.linear_model import LogisticRegression\n\ncores = multiprocessing.cpu_count()\nw2v_model = Word2Vec(window=10,\n                     size = 256,\n                     sg=1)\nw2v_model.build_vocab(sentences)\n\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=25, report_delay=1)\n\n#for epoch in range(25):\n    #w2v_model.train(utils.shuffle([x for x in tqdm(X)]), total_examples=len(X), epochs=1)\n    #w2v_model.alpha -= 0.002\n    #w2v_model.min_alpha = model_ug_dbow.alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_vectors(model, corpus, size):\n    vecs = np.zeros((len(corpus), size))\n    n = 0\n    for i in corpus.index:\n        vecs[i] = np.zeros(size).reshape((1, size))\n        for word in str(corpus[0][i]).split():\n            try:\n                vecs[i] += model[word]\n                n += 1\n            except KeyError:\n                continue\n                \n    return vecs\n  \n\n\n#train_vecs_dbow = get_vectors(w2v_model.train., X, 256)\n#validation_vecs_dbow = get_vectors(w2v_model, x_validation, 100)\n\n#clf = LogisticRegression()\n#clf.fit(train_vecs_dbow, y_train)\n#clf.score(validation_vecs_dbow, y_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences.corpus[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=pd.DataFrame(X) # numpy objects have no split() attribute ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sg = get_vectors(w2v_model, X, 256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sg=pd.DataFrame(X_sg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nscale = preprocessing.normalize\nX_sg=scale(X_sg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sg=pd.DataFrame(X_sg)+1\nX_sg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df=X_sg\nnew_df[\"class\"]=y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sg_n = new_df[new_df['class']==0]\nX_sg_p = new_df[new_df['class']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sg_n = X_sg_n.iloc[:,0:256]\nX_sg_p = X_sg_p.iloc[:,0:256]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sg_n.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_p = y[df['class']==1]\ny_n = y[df['class']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_p_train, X_p_test,y_p_train,y_p_test = train_test_split(X_sg_p, y_p, test_size=0.2, random_state=seed)\n\nX_n_train, X_n_test,y_n_train,y_n_test = train_test_split(X_sg_n, y_n, test_size=0.2, random_state=seed)\n\nX_p_train=pd.DataFrame(X_p_train)\nX_n_train=pd.DataFrame(X_n_train)\nX_train=X_p_train.append(X_n_train)  \n\ny_p_train=pd.DataFrame(y_p_train)\ny_n_train=pd.DataFrame(y_n_train)\ny_train=y_n_train.append(y_p_train)\n\nX_p_test=pd.DataFrame(X_p_test)\nX_n_test=pd.DataFrame(X_n_test)\nX_test=X_p_test.append(X_n_test)\n\ny_p_test=pd.DataFrame(y_p_test)\ny_n_test=pd.DataFrame(y_n_test)\ny_test=y_p_test.append(y_n_test)\n\n\n#batch_size = 128\n#X_train1, X_valid, y_train1, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.vector_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_dim = 256\nlstm_out = 64\nbatch_size = 64\n\n\nmodel = Sequential()\nmodel.add(Embedding(2000, 64,  input_length = 256 ))\nmodel.add(LSTM(lstm_out, dropout=0.5, recurrent_dropout=0.5))\nmodel.add(Dense(1, activation='tanh'))\nmodel.compile(loss='binary_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\nmodel_history = model.fit(X_train, y_train, epochs=50, batch_size=batch_size, verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores=model.evaluate(X_test, y_test)\nprint(scores[1]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(20000, embedding_size, input_length=max_words))\nmodel.add(keras.layers.GRU(128))\nmodel.add(Dense(1, activation='tanh'))\nmodel.compile(loss='binary_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\nmodel_history = model.fit(X_train1, y_train1, validation_data=(X_valid, y_valid),epochs=epochs, batch_size=batch_size, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores=model.evaluate(validation_vecs_cbowsg_sum, y_validation,verbose=0)\nprint(scores[1]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Malheureseument à cause du temps on n'a pas pu generer les rendements pour sentiments\nNi les types de régressions prédictives pour les séries temporelles de retour de journal d’index de crypto-monnaie (la moyenne autorégressive et la variance)."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}